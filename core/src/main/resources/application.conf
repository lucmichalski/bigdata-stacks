sparkMaster = "local[*]"
hdfsDataPath = "hdfs://localhost:9000/spark/data"
streamingBatchDurationSeconds = 10
streamingWindowDurationSeconds = 30
checkpointDir = "hdfs://localhost:9000/spark/checkpoints"
hllBitsCount = 12
batchBucketMinutes = 60

cassandra {
  port = 9042
  host = "127.0.0.1"
  keepAliveMs = 15000
  userName = "cassandra"
  keyspaceName = "lambda"
  tables = [
    "batch_unique_visitors_by_site",
    "batch_actions_by_site",
    "speed_unique_visitors_by_site",
    "speed_actions_by_site"
  ]
}

postgres {
  connectionString = "jdbc:postgresql://localhost:5432/lambda?user=postgres&password=mysecretpassword"
  tables = [
    "rdbms_site"
    "rdbms_action_type",
    "rdbms_visitor",
    "rdbms_action",
  ]
}

kafka {
  topic = "lambda"
  numOfPartitions = 1

  producer {
    clientId = "LambdaDataProducer"
    bootstrapServers = "localhost:9092"
    acks = "all"
    maxRetries = 3
    batchSizeBytes = 1638
    lingerTimeMs = 1
    bufferSizeBytes = 33554432
    keySerializerClass = "org.apache.kafka.common.serialization.StringSerializer"
    valueSerializerClass = "org.apache.kafka.common.serialization.StringSerializer"
  }

  streamConsumer {
    clientId = "StreamConsumer"
    groupId = "stream_consumer"
    bootstrapServers = "localhost:9092"
    autoOffsetReset = "largest"
    keyDeserializerClass = "org.apache.kafka.common.serialization.StringDeserializer"
    valueDeserializerClass = "org.apache.kafka.common.serialization.StringDeserializer"
  }
}

trafficGen {
  recordsPerBatch = 1000000
  numOfBatches = 10000
  numOfVisitors = 1000000
  timeMultiplier = 1
  numOfSubpages = 15
  sleepAfterEachFileMs = 2000
}

